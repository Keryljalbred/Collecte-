# ğŸ§© Projet : Pipeline de collecte et dâ€™analyse de logs avec Apache Airflow & MongoDB

## ğŸ“˜ Contexte

Une entreprise souhaite surveiller et analyser les activitÃ©s de ses serveurs web afin de :
- DÃ©tecter les erreurs,
- Suivre les performances,
- Comprendre les tendances dâ€™utilisation.

Les logs gÃ©nÃ©rÃ©s quotidiennement par les serveurs doivent Ãªtre :
1. CollectÃ©s automatiquement,
2. NettoyÃ©s et structurÃ©s,
3. StockÃ©s efficacement pour Ãªtre analysÃ©s.

---

## ğŸ¯ Objectifs du projet

### 1. Collecte des logs
- RÃ©cupÃ©rer les logs depuis diffÃ©rentes sources (serveurs web, applications, etc.).
- Automatiser lâ€™extraction des fichiers de log quotidiennement.

### 2. Nettoyage et transformation
- Filtrer les logs inutiles (fichiers CSS, images, etc.).
- Extraire les champs clÃ©s : **adresse IP, date/heure, URL, code HTTP, User-Agent**.
- Structurer les donnÃ©es en **JSON** pour MongoDB.

### 3. Stockage NoSQL
- InsÃ©rer les logs nettoyÃ©s dans **MongoDB**.
- CrÃ©er des index pour accÃ©lÃ©rer les recherches.

### 4. Orchestration avec Apache Airflow
- Automatiser les Ã©tapes de collecte, traitement et stockage.
- Superviser les exÃ©cutions et gÃ©rer les erreurs automatiquement.

---

## ğŸ› ï¸ Stack technique

| Technologie     | RÃ´le                                                                 |
|----------------|----------------------------------------------------------------------|
| ğŸ Python       | Lecture, nettoyage, transformation des logs                          |
| ğŸ—ƒï¸ MongoDB      | Stockage NoSQL des logs structurÃ©s                                    |
| ğŸª‚ Apache Airflow | Orchestration du pipeline ETL                                        |
| ğŸ“Š Power BI     | Visualisation des donnÃ©es exportÃ©es                                   |
| ğŸ³ Docker        | Conteneurisation des services pour un dÃ©ploiement reproductible      |

---


---

## ğŸ“¥ Sources de logs utilisÃ©es

- [Apache HTTP Logs](https://httpd.apache.org/docs/2.4/logs.html)
- [Fake Apache Log Generator](https://github.com/kiritbasu/Fake-Apache-Log-Generator)
- [Common Crawl Logs](https://commoncrawl.org/)

---

## ğŸ§ª Ã‰tapes du projet

1. **DÃ©finition de la source**
   - Configuration dâ€™un gÃ©nÃ©rateur de logs ou import depuis un serveur rÃ©el.

2. **Ingestion**
   - Extraction automatique via Airflow (quotidiennement).

3. **Nettoyage & transformation**
   - Suppression des lignes non pertinentes.
   - Standardisation des donnÃ©es (JSON).
   - Enrichissement possible (gÃ©olocalisation IP, catÃ©gories HTTP).

4. **Stockage MongoDB**
   - Insertion dans `logs_db.cleaned_logs`.
   - Indexation sur les champs clÃ©s (ex: IP, date, code HTTP).

5. **Automatisation avec Airflow**
   - DAG avec 3 tÃ¢ches : `ingest_logs`, `clean_logs`, `store_logs`.
   - Planning quotidien, logs dâ€™exÃ©cution et alertes.

6. **Export des donnÃ©es**
   - Export automatique des logs depuis MongoDB vers un fichier `logs_export.csv` (via script Python).
   - Fichier utilisÃ© comme **source de donnÃ©es Power BI**.

---

## ğŸ“Š Visualisation avec Power BI

- Le fichier `logs_export.csv` gÃ©nÃ©rÃ© peut Ãªtre importÃ© dans Power BI Desktop.
- Exemples de visualisations :
  - ğŸ“ˆ Ã‰volution du nombre de requÃªtes HTTP dans le temps
  - ğŸŒ RÃ©partition des adresses IP (trafic par pays)
  - ğŸ§¾ Top des pages demandÃ©es
  - â— Analyse des erreurs (codes HTTP 4xx, 5xx)
  - ğŸ“± Analyse des User-Agent (navigateurs / OS les plus utilisÃ©s)

### Ã‰tapes dâ€™intÃ©gration Power BI :
1. Ouvrir Power BI Desktop
2. Importer `logs_export.csv`
3. CrÃ©er des visualisations avec les champs : `ip`, `date`, `request`, `status`, `user_agent`, etc.
4. Mettre en forme les graphiques et ajouter des filtres dynamiques.
5. Optionnel : programmer une **actualisation automatique** du fichier CSV.

---

lla/5.0 ..."
}

ğŸ“« Contact
Projet rÃ©alisÃ© dans un cadre pÃ©dagogique.

Auteur : [BJEUKOUA KERYL]

Email : [keryljk@gmail.com]


